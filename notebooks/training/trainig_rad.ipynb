{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add repo path to the system path\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "repo_path= Path.cwd().resolve()\n",
    "while '.gitignore' not in os.listdir(repo_path): # while not in the root of the repo\n",
    "    repo_path = repo_path.parent #go up one level\n",
    "sys.path.insert(0,str(repo_path)) if str(repo_path) not in sys.path else None\n",
    "\n",
    "# #sklearnex\n",
    "# from sklearnex import patch_sklearn\n",
    "# patch_sklearn()\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import accuracy_score\n",
    "import cv2 as cv\n",
    "from radiomics import featureextractor\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Import paths and patients classes\n",
    "from notebooks.info import path_label, patient\n",
    "import notebooks.utils as utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor_settings(param_path, show=False):\n",
    "    \"\"\"set extraction settings for pyradiomics\n",
    "\n",
    "    Args:\n",
    "        param_path (str): relative path of parameter file\n",
    "        show (bool, optional): if printing setting or not. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        obj: extractor of pyradiomics\n",
    "    \"\"\"\n",
    "    extractor = featureextractor.RadiomicsFeatureExtractor(str(repo_path /param_path))\n",
    "    if show:\n",
    "        print('Extraction parameters:\\n\\t', extractor.settings)\n",
    "        print('Enabled filters:\\n\\t', extractor.enabledImagetypes)\n",
    "        print('Enabled features:\\n\\t', extractor.enabledFeatures)\n",
    "    return extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(df: pd.DataFrame, pat: object, rad: str, time: int, stype: str, i: int):\n",
    "    im = sitk.JoinSeries(pat.im_sitk('SET')) #Add dimension to be able to use pyradiomics. Image (a,b) turns into (a,b,1)\n",
    "    #get radiomics features\n",
    "    param_path = 'data/param_files/Param_64bin_all_radiomics.json' #path of parameter file\n",
    "    extractor = extractor_settings(param_path, show=False)\n",
    "    #mask path\n",
    "    mask_path = str(repo_path / pat.seg_path(rad, time, stype)[0])\n",
    "    #extract\n",
    "    result = extractor.execute(im,mask_path) # Extract features\n",
    "    #feature vector length\n",
    "    fv_len = 102\n",
    "    print(list(result.values())[-fv_len:])\n",
    "    #create df is not yet defined\n",
    "    if df is None:\n",
    "        column_names = list(result.keys())[-fv_len:] #get column names\n",
    "        column_names = [x.replace('original_','') for x in column_names] #remove original_ string\n",
    "        column_names.insert(0, 'pat_num') #insert pat_num at the beginning\n",
    "        df = pd.DataFrame(columns=column_names)\n",
    "    #add feature vector to df if it has values\n",
    "    feature_vector = list(result.values())[-fv_len:] #get feature vector\n",
    "    feature_vector.insert(0, pat.pat_num) #insert pat_num at the beginning\n",
    "    #add feature vector according to loc\n",
    "    df.loc[i] = feature_vector\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:09<00:00,  3.39it/s]\n"
     ]
    }
   ],
   "source": [
    "info = path_label()\n",
    "\n",
    "# segmentation type\n",
    "stype='G'\n",
    "general_df = None\n",
    "for rad in ['L', 'V', 'M']:\n",
    "    for time in [1,2]:\n",
    "        df = None # df to store all feature vectors\n",
    "        for i in tqdm(range(info.len)): # go thourgh all patients\n",
    "            pat = patient(info, num=i)\n",
    "            df = feature_extraction(df, pat, rad, time, stype, i)\n",
    "            clear_output(wait=True)\n",
    "        general_df = pd.concat([general_df, df], ignore_index=True)\n",
    "        df.to_csv(repo_path / 'data' / 'features' / f'features_{rad}_{time}_{stype}.csv')\n",
    "general_df.to_csv(repo_path / 'data' / 'features' / f'features_all_time{stype}.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ex_included(budget_path: Path):\n",
    "    # get the name of the features from the budget\n",
    "    budget = pd.read_excel( budget_path, index_col=0)\n",
    "    # change name of column\n",
    "    budget.columns = ['budget']\n",
    "    # get all features with values greater than 1\n",
    "    excluded = budget[budget[ 'budget' ] > 1].index\n",
    "    # get all other names\n",
    "    included = budget[budget[ 'budget' ] <= 1].index\n",
    "    return excluded, included\n",
    "\n",
    "def get_features(stype:str, excluded:list):\n",
    "    # get features\n",
    "    features = pd.read_csv(repo_path / 'data' / 'features' / f'features_all_time{stype}.csv', index_col=0)\n",
    "    features = features.groupby(by='pat_num', axis=0).mean()\n",
    "    # remove features in excluded list\n",
    "    features = features.drop(excluded, axis=1)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HP\n",
    "stype='G' # segmentation type\n",
    "label = 'RP' # receptor type (RP, RE, ki67\n",
    "budget_path = repo_path/ 'data' / 'budget' / 'budget.xlsx'\n",
    "\n",
    "excluded, _ = get_ex_included(budget_path) # get excluded features due to their budget value\n",
    "features = get_features(stype, excluded) \n",
    "info = path_label()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 features with the largest coefficients are:\n",
      "firstorder_InterquartileRange                0.008038\n",
      "firstorder_RobustMeanAbsoluteDeviation       0.007211\n",
      "firstorder_MeanAbsoluteDeviation             0.006907\n",
      "gldm_SmallDependenceHighGrayLevelEmphasis    0.004517\n",
      "glcm_SumAverage                              0.002541\n",
      "glszm_SmallAreaHighGrayLevelEmphasis         0.001575\n",
      "firstorder_Range                             0.001543\n",
      "firstorder_Variance                          0.001179\n",
      "glrlm_ShortRunHighGrayLevelEmphasis          0.001065\n",
      "glcm_Autocorrelation                         0.001062\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_features = 10\n",
    "\n",
    "# use lasso to select features\n",
    "lasso = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)\n",
    "lasso.fit(features, info.labels_list(label))\n",
    "\n",
    "# use lasso to select the 5 features with the largest coefficients\n",
    "coef = pd.Series(lasso.coef_[0], index=features.columns)\n",
    "coef = coef.sort_values(ascending=False)\n",
    "coef = coef[coef != 0]\n",
    "coef = coef[:num_features]\n",
    "print(f'The {num_features} features with the largest coefficients are:\\n{coef}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8181818181818182\n",
      "A dumb prediction would have an accuracy of: 0.6363636363636364\n"
     ]
    }
   ],
   "source": [
    "x = features[coef.index]\n",
    "# scale features using standard scaler\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "y = info.labels_list(label)\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(x, y)\n",
    "# get the accuracy of the model\n",
    "print(f'Accuracy: {logreg.score(x, y)}')\n",
    "print(f'A dumb prediction would have an accuracy of: {np.mean(y)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ricardino/Documents/MAIA/tercer_semestre/Extra_activities/INCan/Added_value_feature_uncertainty/notebooks/training/trainig_rad.ipynb Cell 18\u001b[0m in \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ricardino/Documents/MAIA/tercer_semestre/Extra_activities/INCan/Added_value_feature_uncertainty/notebooks/training/trainig_rad.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(y)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ricardino/Documents/MAIA/tercer_semestre/Extra_activities/INCan/Added_value_feature_uncertainty/notebooks/training/trainig_rad.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m train_index, test_index \u001b[39min\u001b[39;00m loo\u001b[39m.\u001b[39msplit(features):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ricardino/Documents/MAIA/tercer_semestre/Extra_activities/INCan/Added_value_feature_uncertainty/notebooks/training/trainig_rad.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ricardino/Documents/MAIA/tercer_semestre/Extra_activities/INCan/Added_value_feature_uncertainty/notebooks/training/trainig_rad.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     X_train, X_test \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39miloc[train_index][coef\u001b[39m.\u001b[39mindex], features\u001b[39m.\u001b[39miloc[test_index][coef\u001b[39m.\u001b[39mindex]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ricardino/Documents/MAIA/tercer_semestre/Extra_activities/INCan/Added_value_feature_uncertainty/notebooks/training/trainig_rad.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     y_train, y_test \u001b[39m=\u001b[39m y[train_index], y[test_index]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "# run a lasso classifier with all the features using leave one out cross validation\n",
    "# LOOCV\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(features)\n",
    "scores = []\n",
    "y = np.array(y)\n",
    "for train_index, test_index in loo.split(features):\n",
    "    print(f'{i+1}')\n",
    "    X_train, X_test = features.iloc[train_index][coef.index], features.iloc[test_index][coef.index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    clf = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    scores.append(accuracy_score(y_test, y_pred))\n",
    "print(f'Accuracy: {np.mean(scores)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avfu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
