{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, sys\n",
    "repo_path= Path.cwd().resolve()\n",
    "while '.gitignore' not in os.listdir(repo_path): # while not in the root of the repo\n",
    "    repo_path = repo_path.parent #go up one level\n",
    "sys.path.insert(0,str(repo_path)) if str(repo_path) not in sys.path else None\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Import paths and patients classes\n",
    "from notebooks.info import path_label, patient\n",
    "import notebooks.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ex_included(budget_path: Path):\n",
    "    \"\"\"get excluded features name using the budget CV value\n",
    "\n",
    "    Args:\n",
    "        budget_path (Path): path to the csv file with the budget\n",
    "\n",
    "    Returns:\n",
    "        sequences: excluded and included features\n",
    "    \"\"\"\n",
    "    # get the name of the features from the budget\n",
    "    budget = pd.read_excel(budget_path, index_col=0)\n",
    "    # change name of column\n",
    "    budget.columns = ['budget']\n",
    "    # get all features with values greater than 1\n",
    "    excluded = budget[budget[ 'budget' ] > 1].index\n",
    "    # get all other names\n",
    "    included = budget[budget[ 'budget' ] <= 1].index\n",
    "    \n",
    "    return excluded, included\n",
    "\n",
    "def get_features(stype:str, excluded:list):\n",
    "    \"\"\"\n",
    "    return df with features given the segmentation type and excluded features\n",
    "\n",
    "    Args:\n",
    "        stype (str): general or focal\n",
    "        excluded (seq): sequence of excluded features\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: df with features\n",
    "    \"\"\"\n",
    "    # get features\n",
    "    features = pd.read_csv(repo_path / 'data' / 'features' / f'features_all_time{stype}.csv', index_col=0)\n",
    "    features = features.groupby(by='pat_num', axis=0).mean()\n",
    "    # remove features in excluded list\n",
    "    features = features.drop(excluded, axis=1)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load info class\n",
    "info = path_label()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV greater than 100 (Uncompatible budget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment HP\n",
    "stype='G' # segmentation type\n",
    "label = 'RP' # receptor type (RP, RE, ki67)\n",
    "\n",
    "# load features\n",
    "budget_path = repo_path/ 'data' / 'budget' / 'budget.xlsx'\n",
    "excluded, _ = get_ex_included(budget_path) # get excluded features due to their budget value\n",
    "features = get_features(stype, excluded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCC\n",
    "\n",
    "Pearson Correlation Coefficient (PCC) based filtering is performed on the features to:\n",
    "- Remove the redundant features\n",
    "- Select those with more stable variability according to the budget values.\n",
    "    - Stability can be seen as a feature-specific variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criteria to group features\n",
    "pcc_value = 0.95\n",
    "\n",
    "# Compute the pearson correlation coefficient\n",
    "pcc = features.corr(method='pearson')\n",
    "# compute absolute value, because we are interested in the magnitude of the correlation, not the sign\n",
    "pcc = pcc.abs()\n",
    "# if the value is greater than 0.9 but not exaclty 1, group them\n",
    "pcc = pcc[(pcc > pcc_value) & (pcc < 1)]\n",
    "pcc.to_csv('pcc_high.csv')\n",
    "# go through each column and get the name of the features that are correlated\n",
    "pcc_names = pcc.apply(lambda x: x.dropna().index.tolist(), axis=1)\n",
    "pcc_names.to_csv('pcc_names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of clusters is 31\n",
      "The cluster leaders are: ['shape2D_Elongation', 'shape2D_MaximumDiameter', 'shape2D_PixelSurface', 'shape2D_MinorAxisLength', 'shape2D_PerimeterSurfaceRatio', 'shape2D_Sphericity', 'firstorder_10Percentile', 'firstorder_Median', 'firstorder_Entropy', 'firstorder_RobustMeanAbsoluteDeviation', 'firstorder_Kurtosis', 'firstorder_Maximum', 'firstorder_Minimum', 'firstorder_Range', 'glcm_SumAverage', 'glcm_Imc1', 'glcm_Idmn', 'glcm_Imc2', 'glcm_JointEntropy', 'glcm_MCC', 'glrlm_RunEntropy', 'glrlm_GrayLevelVariance', 'gldm_LargeDependenceLowGrayLevelEmphasis', 'gldm_LowGrayLevelEmphasis', 'glszm_SmallAreaHighGrayLevelEmphasis', 'glrlm_LongRunHighGrayLevelEmphasis', 'glrlm_LongRunLowGrayLevelEmphasis', 'ngtdm_Coarseness', 'ngtdm_Contrast', 'gldm_SmallDependenceHighGrayLevelEmphasis', 'gldm_SmallDependenceLowGrayLevelEmphasis']\n"
     ]
    }
   ],
   "source": [
    "# list for the clusters (as lists)\n",
    "clusters = []\n",
    "\n",
    "# first example in column\n",
    "for column in pcc_names.index:\n",
    "    # check if list is empty\n",
    "    if not pcc_names[column]: # if empty\n",
    "        clusters.append([column]) # lone feature\n",
    "    else: # if not empty\n",
    "        # make a list including the first feature and all the features that are correlated to it\n",
    "        connected_features = [column] + pcc_names[column]\n",
    "        # check if any of the features in connected features are already in a cluster\n",
    "        for cluster in clusters:\n",
    "            if any(feature in cluster for feature in connected_features):\n",
    "                # if so, add all the features to that cluster\n",
    "                cluster.extend(connected_features)\n",
    "                break\n",
    "        else: # if not, create a new cluster\n",
    "            clusters.append(connected_features)\n",
    "# remove duplicates\n",
    "for i, cluster in enumerate(clusters):\n",
    "    cluster = list(set(cluster))\n",
    "    clusters[i] = cluster\n",
    "\n",
    "print(f'The number of clusters is {len(clusters)}')\n",
    "\n",
    "# select a leader feature for each cluster depending on the budget value\n",
    "budget = pd.read_excel(budget_path, index_col=0).T\n",
    "cluster_leaders = []\n",
    "# example\n",
    "for cluster in clusters:\n",
    "    # get the feature with the lowest budget value\n",
    "    leader = budget[cluster].min().idxmin()\n",
    "    # add it to the list\n",
    "    cluster_leaders.append(leader)\n",
    "\n",
    "print(f'The cluster leaders are: {cluster_leaders}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avfu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
