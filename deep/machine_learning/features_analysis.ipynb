{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add repo path to the system path\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "repo_path= Path.cwd().resolve()\n",
    "while '.gitignore' not in os.listdir(repo_path): # while not in the root of the repo\n",
    "    repo_path = repo_path.parent #go up one level\n",
    "sys.path.insert(0,str(repo_path)) if str(repo_path) not in sys.path else None\n",
    "\n",
    "from utils import dataset_INCan\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "# normalize the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# classifiers\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# use leave one out cross validation\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "# metrics\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create predictor class\n",
    "class predictor_machine():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.clf = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.cv = LeaveOneOut()\n",
    "        self.original_features = None\n",
    "        self.features = None\n",
    "        self.num_samples = 0\n",
    "        self.receptor = None\n",
    "        self.labels = None\n",
    "        # preprocessing\n",
    "        self.corr_threshold = 0.95\n",
    "        # lists\n",
    "        self.pos_probabilities = []\n",
    "        self.true_labels = []\n",
    "        self.best_estimators = []\n",
    "        # budget\n",
    "        self.budget = pd.read_csv(repo_path / 'data/budget/budget_std.csv', index_col=0).mean(axis=0)\n",
    "        self.synthetic_units = 1\n",
    "        self.X_test_augmented = None\n",
    "\n",
    "    def set_classifier(self, pred:sklearn.base.BaseEstimator, parameters:dict, verbose:int=0):\n",
    "        # grid search, the best model is selected based on the roc_auc score\n",
    "        self.clf = GridSearchCV(pred, parameters, cv=5, scoring='roc_auc', verbose=verbose, n_jobs=6, return_train_score=True)\n",
    "\n",
    "    def prepare_features(self, features:pd.DataFrame, show_info=True, scale_together=False):\n",
    "        \"\"\"with the defined scaler, scale feature and return as a dataframe\n",
    "\n",
    "        Args:\n",
    "            features (pd.DataFrame): input features, in order of prediction. Only numerical values.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: scaled features\n",
    "        \"\"\"\n",
    "        # ensure index is in order\n",
    "        # features = features.sort_index()\n",
    "        self.original_features = features.copy()\n",
    "        if show_info:\n",
    "            print(f'Original features, {features.shape[1]} features.')\n",
    "        # eliminate highly correlated features\n",
    "        features = self.eliminate_highly_correlated(features)\n",
    "        if scale_together:\n",
    "            self.scaler.fit(features)\n",
    "            features = pd.DataFrame(self.scaler.transform(features))\n",
    "            # show worning\n",
    "            print('Features scaled together. This could represent DATA LEAKAGE.')\n",
    "        self.num_samples = len(features)\n",
    "        if show_info:\n",
    "            print(f'Features prepared, {self.num_samples} samples, {features.shape[1]} features.')\n",
    "        self.features = features\n",
    "\n",
    "    def eliminate_highly_correlated(self, features:pd.DataFrame):\n",
    "        # find zeros\n",
    "        features = features.loc[:, (features != 0).any(axis=0)]\n",
    "        # compute the correlation matrix\n",
    "        corr = features.corr().abs()\n",
    "        # get the upper triangle of the correlation matrix\n",
    "        upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "        # find features with correlation greater than threshold\n",
    "        to_drop = [column for column in upper.columns if any(upper[column] > self.corr_threshold)]\n",
    "        # drop features\n",
    "        features = features.drop(to_drop, axis=1)\n",
    "        print(f'Features with high correlation dropped, {features.shape[1]} features remaining.')\n",
    "        \n",
    "        return features\n",
    "        \n",
    "\n",
    "    def set_receptor(self, receptor:str, show_distribution:bool=False):\n",
    "        \"\"\"set the receptor to predict, it prepare the labels format\n",
    "\n",
    "        Args:\n",
    "            receptor (str): name of the receptor to predict\n",
    "            show_distribution (bool, optional): show the positive distribution. Defaults to False.\n",
    "        \"\"\"\n",
    "        self.receptor = receptor\n",
    "        labels = dataset_INCan().labels_list(receptor=self.receptor)\n",
    "        self.labels = pd.DataFrame(labels, columns=[self.receptor])\n",
    "        if show_distribution:\n",
    "            print(f'The positive cases of {self.receptor} represent {self.labels[self.receptor].mean().round(3)*100}%')  \n",
    "\n",
    "    def train(self):\n",
    "        for train_index, test_index in self.cv.split(self.features):\n",
    "            # get the train and test data\n",
    "            X_train, X_test = self.features.iloc[train_index], self.features.iloc[test_index]\n",
    "            # scale the data\n",
    "            X_train = self.scaler.fit_transform(X_train)\n",
    "            X_test = self.scaler.transform(X_test)\n",
    "            # get the train and test labels\n",
    "            y_train, y_test = self.labels.iloc[train_index], self.labels.iloc[test_index]\n",
    "            # fit the model\n",
    "            self.clf.fit(X_train, y_train.values.ravel())\n",
    "            # save best params of this iteration\n",
    "            self.best_estimators.append(self.clf.best_estimator_)\n",
    "            # predict the probability\n",
    "            y_prob = self.clf.predict_proba(X_test)[0,1] # get only the positive class\n",
    "            # append the prediction\n",
    "            self.pos_probabilities.append(y_prob)\n",
    "            self.true_labels.append(y_test.values[0,0])\n",
    "        # convert to arrays\n",
    "        self.true_labels = np.asarray(self.true_labels)\n",
    "        self.pos_probabilities = np.asanyarray(self.pos_probabilities)\n",
    "\n",
    "        print(f'Training finished!')\n",
    "\n",
    "    def compute_metrics(self, plot_metrics:bool=True):\n",
    "        \n",
    "        roc_auc = roc_auc_score(self.true_labels, self.pos_probabilities)\n",
    "        fpr, tpr, thresholds = roc_curve(self.true_labels, self.pos_probabilities)\n",
    "        # get ideal threshold\n",
    "        optimal_idx = np.argmax(tpr - fpr)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        # get optimal predictions\n",
    "        optimal_predictions = np.where(self.pos_probabilities > optimal_threshold, 1, 0) # when to predict positive\n",
    "        accuracy = accuracy_score(self.true_labels, optimal_predictions)\n",
    "        precision = precision_score(self.true_labels, optimal_predictions)\n",
    "        recall = recall_score(self.true_labels, optimal_predictions)\n",
    "        f1 = f1_score(self.true_labels, optimal_predictions)\n",
    "        if not plot_metrics:\n",
    "            print(f'\\nAUC:{roc_auc}\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1: {f1}\\n')\n",
    "        if plot_metrics:\n",
    "            # plot confusion matrix on the left\n",
    "            fig, ax = plt.subplots(1,3, figsize=(15,5))\n",
    "            cm = confusion_matrix(self.true_labels, optimal_predictions)\n",
    "            ConfusionMatrixDisplay(cm).plot(ax=ax[0])\n",
    "            ax[0].set_title('Confusion Matrix')\n",
    "            # plot the ROC curve on the right\n",
    "            ax[1].plot(fpr, tpr, label=f'ROC curve (area = {roc_auc})')\n",
    "            ax[1].plot([0,1],[0,1], 'k--')\n",
    "            ax[1].set_xlabel('False Positive Rate')\n",
    "            ax[1].set_ylabel('True Positive Rate')\n",
    "            ax[1].set_title('ROC Curve')\n",
    "            ax[1].scatter(fpr[optimal_idx], tpr[optimal_idx], marker='o', color='black', label=f'Best Threshold: {optimal_threshold.round(3)}')\n",
    "            ax[1].legend()\n",
    "            # plot the metrics on the right, using thin bars\n",
    "            ax[2].barh(['Accuracy', 'Precision', 'Recall', 'F1'], [accuracy, precision, recall, f1], height=0.5)\n",
    "            # set the limits\n",
    "            ax[2].set_xlim(0,1)\n",
    "            ax[2].set_xticks(np.arange(0,1.1,0.1))\n",
    "            # write the values on the bars\n",
    "            for i, v in enumerate([accuracy, precision, recall, f1]):\n",
    "                ax[2].text(v+0.01, i-0.1, f'{v.round(2)}')\n",
    "                \n",
    "            ax[2].set_title('Metrics')\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "    def dumb_inference(self):\n",
    "        \"\"\"infere with a feature vector of zeros\"\"\"\n",
    "        # restart the lists\n",
    "        self.probabilities = []\n",
    "        self.true_labels = []\n",
    "        # create a dumb feature vector\n",
    "        dumb_features = pd.DataFrame(np.zeros((self.features.shape[0], self.features.shape[1])))\n",
    "        # predict\n",
    "        for _, test_index in self.cv.split(self.features):\n",
    "            # get the train and test data\n",
    "            X_test = dumb_features.iloc[test_index]\n",
    "            y_test = self.labels.iloc[test_index]\n",
    "            # predict the probability\n",
    "            y_prob = self.best_estimators[test_index[0]].predict_proba(X_test)[0,1] # get only the positive class\n",
    "            # append the prediction\n",
    "            self.probabilities.append(y_prob)\n",
    "            self.true_labels.append(y_test.values[0,0]) \n",
    "        # convert to arrays\n",
    "        self.true_labels = np.asarray(self.true_labels)\n",
    "        self.probabilities = np.asanyarray(self.probabilities)\n",
    "        print('Dumb prediction, using zero-valued feature vectors, NO training done.')\n",
    "\n",
    "    def ordered_inference(self):\n",
    "        # restart the lists\n",
    "        self.probabilities = []\n",
    "        self.true_labels = []\n",
    "        # predict\n",
    "        for _, test_index in self.cv.split(self.features):\n",
    "            # get the train and test data\n",
    "            X_test = self.features.iloc[test_index]\n",
    "            y_test = self.labels.iloc[test_index]\n",
    "            # predict the probability\n",
    "            y_prob = self.best_estimators[test_index[0]].predict_proba(X_test)[0,1] # get only the positive class\n",
    "            # append the prediction\n",
    "            self.probabilities.append(y_prob)\n",
    "            self.true_labels.append(y_test.values[0,0])\n",
    "        # convert to arrays\n",
    "        self.true_labels = np.asarray(self.true_labels)\n",
    "        self.probabilities = np.asanyarray(self.probabilities)\n",
    "        print('Recomputed prediction, NO training done.')\n",
    "\n",
    "    def budget_inference(self):\n",
    "        # restart the lists\n",
    "        self.probabilities = []\n",
    "        self.true_labels = []\n",
    "\n",
    "        # predict\n",
    "        for _, test_index in self.cv.split(self.features):\n",
    "                # repeat the test sample to match the number of synthetic samples\n",
    "                y_test = self.labels.iloc[test_index]\n",
    "                y_test_augmented = np.repeat(y_test, self.synthetic_units, axis=0)\n",
    "\n",
    "                # generate the synthetic samples\n",
    "                X_test_base = self.original_features.iloc[test_index]\n",
    "                self.X_test_augmented = self.augment_test(X_test_base)\n",
    "\n",
    "                # predict the probability\n",
    "                y_prob = self.best_estimators[test_index[0]].predict_proba(self.X_test_augmented)\n",
    "\n",
    "                # append the prediction\n",
    "                self.probabilities.append(y_prob)\n",
    "                self.true_labels.append(y_test_augmented)\n",
    "        # convert to arrays\n",
    "        self.true_labels = np.asarray(self.true_labels).reshape(-1,1)[:,0]\n",
    "        self.probabilities = np.asanyarray(self.probabilities).reshape(-1,2)[:,1]\n",
    "        print('Computed prediction using budge, NO training done.')\n",
    "\n",
    "    def augment_test(self, X_test_base:pd.DataFrame):\n",
    "        \"\"\"given a base test set, augment it with random samples from a gaussian distribution with mean zero and std given by the budget\n",
    "\n",
    "        Args:\n",
    "            X_test_base (pd.DataFrame): base of the augmentation\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: augmented test set\n",
    "        \"\"\"\n",
    "        X_test_base = pd.concat([X_test_base]*self.synthetic_units, ignore_index=True)\n",
    "        # generate 1000 random samples from a gaussian distribution, mean zero and std given by the budget\n",
    "        random_samples = np.random.normal(0, self.budget, size=(self.synthetic_units,1024))\n",
    "        random_samples = pd.DataFrame(random_samples, columns=X_test_base.columns)\n",
    "        # sum the random samples to the base features\n",
    "        X_test_augmented = X_test_base + random_samples\n",
    "\n",
    "        # scale according to scaler\n",
    "        X_test_augmented = pd.DataFrame(self.scaler.transform(X_test_augmented))\n",
    "\n",
    "        # send all the features to 0\n",
    "        X_test_augmented = np.where(X_test_augmented < 0, 0, X_test_augmented)\n",
    "        X_test_augmented = np.where(X_test_augmented > 0, 0, X_test_augmented)\n",
    "        X_test_augmented = pd.DataFrame(X_test_augmented)\n",
    "\n",
    "        return X_test_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The positive cases of RP represent 63.6%\n",
      "Original features, 1024 features.\n",
      "Features with high correlation dropped, 935 features remaining.\n",
      "Features prepared, 33 samples, 935 features.\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "receptor = 'RP'\n",
    "# Input data\n",
    "features = pd.read_csv(repo_path / 'data/deep/features/features_1024/L_2_features.csv', index_col=0)\n",
    "\n",
    "# prepare for the machine\n",
    "predictor = predictor_machine()\n",
    "predictor.set_receptor(receptor, show_distribution=True)\n",
    "\n",
    "pred = RandomForestClassifier()\n",
    "parameters = {'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "              'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "              'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "\n",
    "predictor.set_classifier(pred, parameters, verbose=0)\n",
    "\n",
    "predictor.prepare_features(features)\n",
    "predictor.train()\n",
    "predictor.compute_metrics(plot_metrics=True)\n",
    "predictor.dumb_inference()\n",
    "predictor.compute_metrics(plot_metrics=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cedm-deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
